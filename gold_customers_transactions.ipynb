{"cells":[{"cell_type":"code","source":["# importamos librerias\n","import pyspark.sql.functions as F\n","from pyspark.sql import DataFrame"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"de50318a-a197-426a-9803-500cc16b7058","normalized_state":"finished","queued_time":"2024-09-08T16:00:48.3192873Z","session_start_time":"2024-09-08T16:00:48.5218549Z","execution_start_time":"2024-09-08T16:01:52.3218053Z","execution_finish_time":"2024-09-08T16:01:54.981817Z","parent_msg_id":"d8e166fa-4d9a-4b6d-a202-b9518a46b4d8"},"text/plain":"StatementMeta(, de50318a-a197-426a-9803-500cc16b7058, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1d06aed-0078-4b65-a5f8-6ff953ceaba3"},{"cell_type":"code","source":["# enriquecemos los datos de customers con la info de transacciones\n","\n","# calculamos el monto total trasnsaccionado para la semana en curso y para la semana anterior.F\n","current_week = F.weekofyear(F.current_date())\n","current_year = F.year(F.current_date())\n","last_week = F.when(current_week == 1, 52).otherwise(current_week - 1) # Controlamos por la primera semana del aÃ±o\n","last_year = F.when(current_week == 1, current_year - 1).otherwise(current_year)\n","\n","# cargamos los dfs de transacciones con los datos que necesitamos\n","transactions_current_week = spark.read.format(\"delta\").option(\"inferSchema\", \"true\")\\\n","                                                      .load(\"Files/silver/transactions\")\\\n","                                                      .where((F.weekofyear(F.col(\"date\")) == current_week) & (F.year(F.col(\"date\")) == current_year))\n","\n","\"\"\"\n","transactions_last_week = spark.read.format(\"delta\").option(\"inferSchema\", \"true\")\\\n","                                                      .load(\"Files/silver/transactions\")\\\n","                                                      .where((F.weekofyear(F.col(\"date\")) == last_week) & (F.year(F.col(\"date\")) == last_year))\n","\"\"\"\n","\n","def calculos_transacciones(df: DataFrame, week: str) -> DataFrame:\n","    \"\"\"\n","    \"\"\"\n","    if week not in [\"current\", \"prev\"]:\n","        raise ValueError(f\"El paramentro 'week' debe tomar el valor 'current' o 'prev'. '{week}' no es valido\")\n","\n","    # cantidad total\n","    col_quantity_name = \"total_quantity_\" + week\n","    df_total_quantity = df.groupBy(\"customer_id\").agg(F.sum(F.col(\"quantity\")).alias(col_quantity_name))\n","\n","    # cantidad de trasacciones por tipo\n","    df_transaction_type = df.groupBy(\"customer_id\", \"transfer_type\").agg(F.count(F.col(\"transfer_type\")).alias(\"transaction_count\"))\n","    df_transaction_type_pivot = df_transaction_type.groupBy(\"customer_id\").pivot(\"transfer_type\").sum(\"transaction_count\")\n","    df_transaction_type_pivot = df_transaction_type_pivot.fillna(0)\n","\n","    for col_name in df_transaction_type_pivot.columns:\n","        if col_name != \"customer_id\":\n","            new_col_name = col_name.replace(\" \", \"_\") + \"_\" + week\n","            df_transaction_type_pivot = df_transaction_type_pivot.withColumnRenamed(col_name, new_col_name)\n","    \n","    # cantidad de operaciones con origen o destino en el extranjero\n","    def transaciones_origen_destino(df: DataFrame, column:str, week:str) -> DataFrame:\n","        df_fuera_esp = df.filter(F.col(column) != \"ESP\")\n","        column_name = \"origin_not_esp\" + \"_\" + week if column == \"country_of_origin\" else \"destiny_not_esp\" + \"_\" + week\n","        df_fuera_esp = df_fuera_esp.groupBy(F.col(\"customer_id\")).agg(F.count(\"customer_id\").alias(column_name))\n","        return df_fuera_esp\n","    \n","    df_transaction_origin = transaciones_origen_destino(df, \"country_of_origin\", week)\n","    df_transaction_destiny = transaciones_origen_destino(df, \"country_of_destiny\", week)\n","\n","    df_transactiones = df_total_quantity.join(df_transaction_type_pivot, on=\"customer_id\", how=\"outer\")\\\n","                                        .join(df_transaction_origin, on=\"customer_id\", how=\"outer\")\\\n","                                        .join(df_transaction_destiny, on=\"customer_id\", how=\"outer\")\n","\n","    df_transactiones = df_transactiones.fillna(0)\n","\n","    return df_transactiones\n","\n","# Obtenemos los datos para la semana en curso y la semana anterior\n","transactions_info_current = calculos_transacciones(transactions_current_week, week=\"current\")\n","# transactions_info_prev = calculos_transacciones(transactions_last_week, week=\"prev\")\n","\n","# Enriquecemos el dataset de cliente con la info calculada sobre transacciones\n","customers_df = spark.read.format(\"delta\").option(\"inferSchema\", \"true\").load(\"Files/silver/customers\")\n","customers_df = customers_df.join(transactions_info_current, on=\"customer_id\", how=\"left\")\n","                           #.join(transactions_info_prev, on=\"customer_id\", how=\"left\")\n","customers_df = customers_df.fillna(0)\n","\n","# Creamos el riego final de cliente\n","customer_gold = customers_df.withColumn(\"risk_customer\", F.round(F.col(\"risk_customer\"), 2))\\\n","                            .withColumn(\"risk_customer_d\", F.when(F.col(\"risk_customer\") <= 0.3, \"low\")\n","                                                            .when(F.col(\"risk_customer\") <= 0.35, \"mid\")\n","                                                            .when(F.col(\"risk_customer\") <= 0.5, \"high\")\n","                                                            .otherwise(\"very high\"))\n","\n","# Guardamos la informacion en el lakehouse y creamos la tabla final\n","\n","print(\"Guardando ficheros y creando tabla...\")\n","customer_gold.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(\"Files/gold/customers_enhanced\")\n","customer_gold.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"customers_gold\")\n","print(\"Operacion termianda\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"2c2dc0d8-5108-4a57-ac75-66838739db73","normalized_state":"finished","queued_time":"2024-09-08T15:25:04.8015799Z","session_start_time":null,"execution_start_time":"2024-09-08T15:25:05.141465Z","execution_finish_time":"2024-09-08T15:25:49.4725041Z","parent_msg_id":"990a89dc-2514-449c-be70-667adf5ae88e"},"text/plain":"StatementMeta(, 2c2dc0d8-5108-4a57-ac75-66838739db73, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Guardando ficheros y creando tabla...\nOperacion termianda\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"advisor":{"adviceMetadata":"{\"artifactId\":\"f5a8e9f5-d80c-47e6-bbaf-e8031e661cfe\",\"activityId\":\"2c2dc0d8-5108-4a57-ac75-66838739db73\",\"applicationId\":\"application_1725808644389_0001\",\"jobGroupId\":\"6\",\"advices\":{\"warn\":2}}"}},"id":"37fa592f-8093-4977-b341-1c04c9b45b2b"},{"cell_type":"code","source":["# Creamos la tabla gold de transacciones\n","checkpoint_dir = \"Files/gold/transactions_enhanced/checkpoint\"\n","checkpoint_dir_table = \"Files/gold/transactions_enhanced/checkpoint_table\"\n","gold_transactions_dir = \"Files/gold/transactions_enhanced\"\n","\n","\n","# Cargamos el dataset y lo enriquecemos con el riesgo cliente\n","transaction_df = spark.readStream.format(\"delta\").option(\"inferSchema\", \"true\").load(\"Files/silver/transactions\")\n","customer_df = spark.sql(\"SELECT customer_id, risk_customer, risk_customer_d FROM customers_gold\")\n","transaction_df = transaction_df.join(customer_df, on=\"customer_id\", how=\"left\")\n","transaction_gold = transaction_df.withColumn(\"risk_transactional\", F.round(F.col(\"risk_transactional\"), 2))\n","\n","# Persitimos los datos en la capa gold\n","query_file = transaction_gold.writeStream.outputMode(\"append\").format(\"delta\")\\\n","                                                             .option(\"checkpointLocation\", checkpoint_dir)\\\n","                                                             .option(\"path\", gold_transactions_dir)\\\n","                                                             .option(\"mergeSchema\", \"true\")\\\n","                                                             .trigger(availableNow=True)\\\n","                                                             .start()\n","\n","# Persistir en la tabla de transacciones gold\n","query_transaction = transaction_gold.writeStream.outputMode(\"append\").format(\"delta\")\\\n","                                                             .option(\"checkpointLocation\", checkpoint_dir_table)\\\n","                                                             .option(\"mergeSchema\", \"true\")\\\n","                                                             .trigger(availableNow=True)\\\n","                                                             .toTable(\"transactions_gold\")\n","\n","query_file.awaitTermination()\n","query_transaction.awaitTermination()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"de50318a-a197-426a-9803-500cc16b7058","normalized_state":"finished","queued_time":"2024-09-08T16:02:07.8077613Z","session_start_time":null,"execution_start_time":"2024-09-08T16:02:08.1355848Z","execution_finish_time":"2024-09-08T16:02:22.5577849Z","parent_msg_id":"5d062987-f33b-4369-b2b2-277e700421aa"},"text/plain":"StatementMeta(, de50318a-a197-426a-9803-500cc16b7058, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<pyspark.sql.streaming.query.StreamingQuery at 0x7553c4c3c580>"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b5265ba-997f-4299-a883-09d2110feecd"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"es"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"a7565f6c-536f-46b5-a425-20440465cbf1","default_lakehouse_name":"aml_tm","default_lakehouse_workspace_id":"13c54c9b-b00c-4491-97bb-db55c69dbcba"}}},"nbformat":4,"nbformat_minor":5}